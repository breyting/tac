{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2 Matias Gathoye avec l'année 1898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation de l'extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yake\n",
    "\n",
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix de l'année et sélection des fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une année\n",
    "year = 1898\n",
    "\n",
    "# Lister les Fichiers\n",
    "txt_path = '../data/txt'\n",
    "files = [f for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f)) and str(year) in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des keywords de tous les fichiers \n",
    "(attention, pour 100 fichiers c'est long. Je ne l'ai pas fait mais j'ai vu que ça fonctionnait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(files):\n",
    "    text = open(os.path.join(txt_path, f), 'r', encoding=\"utf-8\").read()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie Wordcloud\n",
    "Il y a des lignes qui seront répétées par rapport au code-ci dessus, comme la déclaration de l'année par exemple. Je les laisse au cas où vous commencez à lancer le code à partir du point 2. Ce sera pareil pour les points suivants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports et stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import os  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords (Idem que dans s1)\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\",\n",
    "       \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\",\n",
    "       \"celle\", \"entre\", \"encore\", \"toutes\", \"pendant\", \"moins\", \"dire\", \"cela\", \"non\",\n",
    "       \"faut\", \"trois\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\",\n",
    "       \"van\", \"het\", \"autre\", \"jusqu\", \"ville\", \"rossel\", \"dem\"]\n",
    "\n",
    "# liste perso en plus\n",
    "sw += [\"où\", \"apparemment\", \"décidément\", \"déjà\", \"alors\", \"toutes\", \"leurs\", \"avant\", \"celui\", \"toute\", \"elles\",\n",
    "       \"dés\", \"très\", \"peu\"]\n",
    "sw = set(sw)\n",
    "\n",
    "# permet de savoir si un mot est dans ma liste de sw\n",
    "print(\"avant\" in sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du fichier avec le texte de tous les journaux de 1898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir une année\n",
    "year = 1898\n",
    "\n",
    "# Lister les fichiers de cette année\n",
    "data_path = '../data'\n",
    "txt_path = '../data/txt'\n",
    "txts = [f for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f)) and str(year) in f]\n",
    "len(txts)\n",
    "\n",
    "# Stocker le contenu de ces fichiers dans une liste\n",
    "content_list = []\n",
    "for txt in txts:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding='utf-8') as f:\n",
    "        content_list.append(f.read())\n",
    "\n",
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.upper() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(year, folder=temp_path)\n",
    "\n",
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r', encoding='utf-8') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "\n",
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie entité nommées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application de la reconnaissance d'entités nommées pour l'année 1898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text = open(\"../data/tmp/1898.txt\", encoding='utf-8').read()[:n]\n",
    "\n",
    "# Traiter le texte\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changement du if pour récuperer les personnes mais aussi les organisations et les lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "for ent in doc.ents:\n",
    "    if (ent.label_ == \"PER\" or ent.label_ == \"ORG\" or ent.label_ == \"LOC\") and len(ent.text) > 3:\n",
    "        people[ent.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rossel apparait 20 fois dans le corpus\n",
      "nn dem apparait 16 fois dans le corpus\n",
      "Bruxelles apparait 12 fois dans le corpus\n",
      "Nord apparait 9 fois dans le corpus\n",
      "Agence Rossel apparait 8 fois dans le corpus\n",
      "O n dem apparait 8 fois dans le corpus\n",
      "Brux apparait 7 fois dans le corpus\n",
      "Tournai apparait 5 fois dans le corpus\n",
      "F ille apparait 5 fois dans le corpus\n",
      "Louvain apparait 5 fois dans le corpus\n",
      "Midi apparait 5 fois dans le corpus\n",
      "Centre apparait 5 fois dans le corpus\n",
      "Anvers apparait 4 fois dans le corpus\n",
      "cerl apparait 4 fois dans le corpus\n",
      "la France apparait 4 fois dans le corpus\n",
      "Belgique apparait 4 fois dans le corpus\n",
      "Ixellcs apparait 3 fois dans le corpus\n",
      "Gand apparait 3 fois dans le corpus\n",
      "Laeken apparait 3 fois dans le corpus\n",
      "rue Gallait apparait 3 fois dans le corpus\n",
      "prop apparait 3 fois dans le corpus\n",
      "Londres apparait 3 fois dans le corpus\n",
      "Laines apparait 3 fois dans le corpus\n",
      "Discr apparait 3 fois dans le corpus\n",
      "avenue Louise apparait 3 fois dans le corpus\n",
      "Bourse apparait 3 fois dans le corpus\n",
      "dist apparait 3 fois dans le corpus\n",
      "Cbarlèroi apparait 2 fois dans le corpus\n",
      "rue de Brabant apparait 2 fois dans le corpus\n",
      "Fonsny apparait 2 fois dans le corpus\n",
      "rue du Collège apparait 2 fois dans le corpus\n",
      "Conseil apparait 2 fois dans le corpus\n",
      "Ixelles apparait 2 fois dans le corpus\n",
      "Communale apparait 2 fois dans le corpus\n",
      "Américaine apparait 2 fois dans le corpus\n",
      "Palais apparait 2 fois dans le corpus\n",
      ",lav apparait 2 fois dans le corpus\n",
      "bourg apparait 2 fois dans le corpus\n",
      "chaussée de Louvain apparait 2 fois dans le corpus\n",
      "Herbes apparait 2 fois dans le corpus\n",
      "rue des Fabriques apparait 2 fois dans le corpus\n",
      "Limite apparait 2 fois dans le corpus\n",
      "Q n dem apparait 2 fois dans le corpus\n",
      "Madeleine apparait 2 fois dans le corpus\n",
      "conn apparait 2 fois dans le corpus\n",
      "Cureghem apparait 2 fois dans le corpus\n",
      "Constantinople apparait 2 fois dans le corpus\n",
      "Beurre apparait 2 fois dans le corpus\n",
      "çuis apparait 2 fois dans le corpus\n",
      "bne d’enf apparait 2 fois dans le corpus\n"
     ]
    }
   ],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiments avec Textblob-FR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprise de la fonction `get_sentiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*abs(subjectivity):.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{100*subjectivity:.0f}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des 10 phrases de 1898 choisies arbitrairement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is neutral and perfectly objective.\n",
      "This text is 18% positive and 20% subjective.\n",
      "This text is 18% positive and 20% subjective.\n",
      "This text is 5% negative and 35% subjective.\n",
      "This text is 2% negative and 50% subjective.\n",
      "This text is 8% negative and 47% subjective.\n",
      "This text is 30% positive and 37% subjective.\n",
      "This text is 4% positive and 35% subjective.\n",
      "This text is 20% positive and 32% subjective.\n",
      "This text is 8% positive and 13% subjective.\n",
      "This text is 90% positive and 90% subjective.\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(\"On croit que ce boulevard recevra le nom d’Esterhazy.\") #1\n",
    "\n",
    "get_sentiment(\"Evidemment e’était mon droit, puisqu’il y avait menace, exhibition de J’arme.\") #2\n",
    "get_sentiment(\"Évidemment c’était mon droit, puisqu’il y avait menace, exhibition de l’arme.\") \n",
    "# j'ai mis  la même phrase avec correction des petites fautes pour voir si ça changeait le résultat. Ce n'est pas le cas apparemment :)\n",
    "\n",
    "get_sentiment(\"satisfait, il allait prononcer;: Ne bougeons plus! quand patatras, l’estrade mal assise s'effondre et les douze hommes s’étalent pèle môle parmi les planches et les trétaux. \") #3\n",
    "get_sentiment(\"L'aventure eut été plutôt joyeuse si quatre malheureux n’étaient restés étendus, les jambes plus ou moins mises à mal.\") #4\n",
    "get_sentiment(\"Informations prises, les victimes avaient eu plus de peur que de mal, heureusement.\") #5\n",
    "get_sentiment(\"Ce n’est pas la première fois,souvenez-vous-en, que cette juridiction modeste et expéditive se signale à l’admiration sympathique du monde de la pédale.\") #6\n",
    "get_sentiment(\"Or, voici quels événements minuscules ont suscité la décision monumentale qui me chiffonne itérativement.\") #7\n",
    "get_sentiment(\"Non, mais c’est effrayant et désespérant tout ensemble de constater comme les plus précieux proverbes sont chaque jour foulés aux pieds ,par ceux qui les premiers devraient s’en souvenir\") #8\n",
    "get_sentiment(\"il n’y a plus de securité morale pour aucun étranger en France.\") #9\n",
    "get_sentiment(\"Je trouve que, sans exagération; ia Tisane des Shakers est un remède merveilleux.\") #10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tac_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
