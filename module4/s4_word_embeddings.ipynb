{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings : le modèle Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement et traitement des phrases du corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un objet qui *streame* les lignes d'un fichier pour économiser de la RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des bigrams\n",
    "\n",
    "Article intéressant sur le sujet : https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'object `phrases` peut être vu comme un large dictionnaire d'expressions multi-mots associées à un score, le *PMI-like scoring*. Ce dictionnaire est construit par un apprentissage sur base d'exemples.\n",
    "Voir les références ci-dessous :\n",
    "- https://arxiv.org/abs/1310.4546\n",
    "- https://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigram_phrases.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il contient de nombreuses clés qui sont autant de termes observés dans le corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15850647"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phrases.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons une clé au hasard :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1q\n"
     ]
    }
   ],
   "source": [
    "key_ = list(bigram_phrases.vocab.keys())[144]\n",
    "print(key_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire indique le score de cette coocurrence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_phrases.vocab[key_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque l'instance de `Phrases` a été entraînée, elle peut concaténer les bigrams dans les phrases lorsque c'est pertinent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion des `Phrases` en objet `Phraser`\n",
    "\n",
    "`Phraser` est un alias pour `gensim.models.phrases.FrozenPhrases`, voir ici https://radimrehurek.com/gensim/models/phrases.html.\n",
    "\n",
    "Le `Phraser` est une version *light* du `Phrases`, plus optimale pour transformer les phrases en concaténant les bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le `Phraser` est un objet qui convertit certains unigrams d'une liste en bigrams lorsqu'ils ont été identifiés comme pertinents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous répétons l'opération en envoyant cette fois la liste de bigrams afin d'extraire les trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matias/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trigram_phrases \u001b[39m=\u001b[39m Phrases(bigram_phraser[sentences])\n",
      "File \u001b[0;32m~/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/tac_venv/lib/python3.11/site-packages/gensim/models/phrases.py:569\u001b[0m, in \u001b[0;36mPhrases.__init__\u001b[0;34m(self, sentences, min_count, threshold, max_vocab_size, delimiter, progress_per, scoring, connector_words)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[39mif\u001b[39;00m sentences \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_vocab(sentences)\n\u001b[1;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m, msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m in \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime()\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/tac_venv/lib/python3.11/site-packages/gensim/models/phrases.py:648\u001b[0m, in \u001b[0;36mPhrases.add_vocab\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Update model parameters with new `sentences`.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[1;32m    616\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m \n\u001b[1;32m    642\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39m# Uses a separate vocab to collect the token counts from `sentences`.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m# This consumes more RAM than merging new sentences into `self.vocab`\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m# directly, but gives the new sentences a fighting chance to collect\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[39m# sufficient counts, before being pruned out by the (large) accumulated\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[39m# counts collected in previous learn_vocab runs.\u001b[39;00m\n\u001b[0;32m--> 648\u001b[0m min_reduce, vocab, total_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learn_vocab(\n\u001b[1;32m    649\u001b[0m     sentences, max_vocab_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_vocab_size, delimiter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdelimiter,\n\u001b[1;32m    650\u001b[0m     progress_per\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogress_per, connector_words\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnector_words,\n\u001b[1;32m    651\u001b[0m )\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_word_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total_words\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab:\n",
      "File \u001b[0;32m~/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/tac_venv/lib/python3.11/site-packages/gensim/models/phrases.py:584\u001b[0m, in \u001b[0;36mPhrases._learn_vocab\u001b[0;34m(sentences, max_vocab_size, delimiter, connector_words, progress_per)\u001b[0m\n\u001b[1;32m    582\u001b[0m vocab \u001b[39m=\u001b[39m {}\n\u001b[1;32m    583\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mcollecting all words and their counts\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 584\u001b[0m \u001b[39mfor\u001b[39;49;00m sentence_no, sentence \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(sentences):\n\u001b[1;32m    585\u001b[0m     \u001b[39mif\u001b[39;49;00m sentence_no \u001b[39m%\u001b[39;49m progress_per \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[1;32m    586\u001b[0m         logger\u001b[39m.\u001b[39;49minfo(\n\u001b[1;32m    587\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mPROGRESS: at sentence #\u001b[39;49m\u001b[39m%i\u001b[39;49;00m\u001b[39m, processed \u001b[39;49m\u001b[39m%i\u001b[39;49;00m\u001b[39m words and \u001b[39;49m\u001b[39m%i\u001b[39;49;00m\u001b[39m word types\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    588\u001b[0m             sentence_no, total_words, \u001b[39mlen\u001b[39;49m(vocab),\n\u001b[1;32m    589\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/tac_venv/lib/python3.11/site-packages/gensim/interfaces.py:177\u001b[0m, in \u001b[0;36mTransformedCorpus.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[39myield\u001b[39;00m transformed\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mfor\u001b[39;49;00m doc \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcorpus:\n\u001b[1;32m    178\u001b[0m         \u001b[39myield\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj[doc]\n",
      "\u001b[1;32m/home/matias/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbackslashreplace\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39myield\u001b[39;00m [unidecode(w\u001b[39m.\u001b[39;49mlower()) \u001b[39mfor\u001b[39;49;00m w \u001b[39min\u001b[39;49;00m wordpunct_tokenize(line)]\n",
      "\u001b[1;32m/home/matias/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb Cell 24\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbackslashreplace\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matias/Documents/1ULB/Cours%202023-2024/Mastic/TAC/tac/module4/s4_word_embeddings.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39myield\u001b[39;00m [unidecode(w\u001b[39m.\u001b[39;49mlower()) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m wordpunct_tokenize(line)]\n",
      "File \u001b[0;32m~/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/tac_venv/lib/python3.11/site-packages/unidecode/__init__.py:66\u001b[0m, in \u001b[0;36munidecode_expect_ascii\u001b[0;34m(string, errors, replace_str)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m string\n\u001b[0;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m _unidecode(string, errors, replace_str)\n",
      "File \u001b[0;32m~/Documents/1ULB/Cours 2023-2024/Mastic/TAC/tac/tac_venv/lib/python3.11/site-packages/unidecode/__init__.py:136\u001b[0m, in \u001b[0;36m_unidecode\u001b[0;34m(string, errors, replace_str)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m             \u001b[39mraise\u001b[39;00m UnidecodeError(\u001b[39m'\u001b[39m\u001b[39minvalid value for errors parameter \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (errors,))\n\u001b[0;32m--> 136\u001b[0m     retval\u001b[39m.\u001b[39mappend(repl)\n\u001b[1;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(retval)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement d'un modèle Word2Vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en 4 threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarque\n",
    "\n",
    "Vous voyez ici que l'entrainement du modèle est parallélisé (sur 4 workers).\n",
    "\n",
    "Lors qu'on parallélise l'entrainement du modèle, 4 modèles \"séparés\" sont entrainés sur environ un quart des phrases.\n",
    "\n",
    "Ensuite, les résultats sont agrégés pour ne plus faire qu'un seul modèle.\n",
    "\n",
    "On ne peut prédire quel worker aura quelle phrase, car il y a des aléas lors de la parallélisation (p. ex. un worker qui serait plus lent, etc.).\n",
    "\n",
    "Du coup, les valeurs peuvent varier légèrement d'un entrainement à l'autre.\n",
    "\n",
    "Mais, globalement, les résultats restent cohérents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauver le modèle dans un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = f\"../data/newspapers.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le modèle en mémoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imprimer le vecteur d'un terme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"ministre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculer la similarité entre deux termes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chercher les mots les plus proches d'un terme donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire des recherches complexes à travers l'espace vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1932ab1d169b4769d1550e799423b6477588e745f266d79d9004c136c81607e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
